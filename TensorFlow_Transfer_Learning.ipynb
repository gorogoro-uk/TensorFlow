{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow Transfer Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPAaX/Sfg1ZWwOen7ySKPXy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gorogoro-uk/TensorFlow/blob/master/TensorFlow_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI6xgmv--23G",
        "colab_type": "text"
      },
      "source": [
        "**Transer Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkIZ8bRv-zFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pretrained Inception Model for Image Classification\n",
        "\n",
        "# download model weight file from Google, save as H5 file\n",
        "INCEP_URL = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "INCEP_WGTS = BASE / 'incep_v3_weights.h5'\n",
        "\n",
        "# instantiate a model object\n",
        "inception_model = InceptionV3(input_shape = (150, 150, 3),\n",
        "                                include_top = False,\n",
        "                                weights = None)\n",
        "\n",
        "# load pre-trianed weights into model object\n",
        "inception_model.load_weights(INCEP_WGTS)\n",
        "\n",
        "# make layers non-trainable\n",
        "for layer in inception_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# define last layer of pre-trained model to use\n",
        "last_layer = inception_model.get_layer('mixed7')\n",
        "last_output = last_layer.output\n",
        "\n",
        "# define customised model head to attached to pre-trained base\n",
        "# weights will be trainable \n",
        "x = tf.keras.layers.Flatten()(last_output)\n",
        "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# join two models together\n",
        "# ready for training\n",
        "horsehuman_model = tf.keras.Model(inception_model.input, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQFOg1ol-2QK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pretrained Word Embeddings for NLP\n",
        "\n",
        "# download pretrained word emeddings as a file from internet\n",
        "GLOVE_FILE = BASE / 'glove.6B.100d.txt'\n",
        "GLOVE_URL = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt'\n",
        "urllib.request.urlretrieve(GLOVE_URL, GLOVE_FILE)\n",
        "\n",
        "# read embeddings from file into embed_index dict {word: embedding_vector}\n",
        "# has embeddings for a huge volume of words\n",
        "embed_index = {}\n",
        "with open(GLOVE_FILE) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embed_index[word] = coefs\n",
        "\n",
        "# prepare embedding matrix of weights for model shape [vocab_size, embedding_dimension]\n",
        "# only get embeddings for words in the word_list for the given task\n",
        "embed_matrix = np.zeros((vocab_size, embed_dim))      \n",
        "for word, i in word_index.items():\n",
        "    embed_vector = embed_index.get(word)\n",
        "    if embed_vector is not None:\n",
        "        embed_matrix[i] = embed_vector\n",
        "\n",
        "# load weights into Embedding layer, make non-trainable\n",
        "# input: [batch, sentence_length]\n",
        "glove_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, \n",
        "                              embed_dim, \n",
        "                              input_length=max_length, \n",
        "                              weights=[embed_matrix], \n",
        "                              trainable=False),\n",
        "                              # output: [batch, sentence_length, embedding_dimension]\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}