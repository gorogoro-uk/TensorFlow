{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow Model Summary.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "podxsRqs-ktg",
        "Ld4QasVf-ohP"
      ],
      "authorship_tag": "ABX9TyNZe9UEM6EHy+vn+j6iRSvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gorogoro-uk/TensorFlow/blob/master/Tensorflow_Model_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlEhTOaVxZGU",
        "colab_type": "text"
      },
      "source": [
        "## **TensorFlow Model Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "podxsRqs-ktg",
        "colab_type": "text"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsGWzgdFxYPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accuracy Callback\n",
        "# check accuracy at end of every epoch and stop training once reached desired level\n",
        "\n",
        "ACCURACY_LIMIT = 0.99\n",
        "# define accuracy callback class\n",
        "class myAccuracyCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > ACCURACY_LIMIT):\n",
        "            print(f\"\\nreached {ACCURACY_LIMIT*100:.3%} accuracy, terminating training.\\n\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "# instantiate callback object\n",
        "acc_callback = myAccuracyCallback()\n",
        "\n",
        "# reference callback objcet in model.fit()\n",
        "mnist_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    callbacks=[acc_callback]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmUNNzrM9FSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Learning Rate Callback\n",
        "# use different learning rate every epoch to find optimal value\n",
        "# plot loss v learning to find lowest stable value\n",
        "\n",
        "\n",
        "# instantiate a LearningRateScheduler\n",
        "# make learning a function of epoch \n",
        "# 1e-8 < learning_rate < 1e-4 (when epoch = 100)\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "\n",
        "# reference learning rate scheduler in model.fit()\n",
        "history = model.fit(dataset, \n",
        "                    epochs=100, \n",
        "                    callbacks=[lr_schedule])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld4QasVf-ohP",
        "colab_type": "text"
      },
      "source": [
        "### Model Types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdnQTj4WDQDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Simple Dense Neural Net\n",
        "\n",
        "# input: [batch, width(28), height(28), channel(1)]\n",
        "mnist_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),  # [batch, width x height x channel(784)] flatten image matrix\n",
        "    tf.keras.layers.Dense(500, activation='relu'),  # [batch, dnn_units(500)]\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # [batch, dnn_units(1)]\n",
        "])\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "mnist_model.compile(optimizer=RMSprop(lr=0.001),\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWSlSIniGvU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Convolutional Neural Net for Image Classification\n",
        "\n",
        "# 2D convolutions to extract features\n",
        "# MaxPooling to downsample and reduce size\n",
        "sign_model = tf.keras.models.Sequential([                                           # [batch, width(28), height(28), channel(1)]\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),  # [batch, filter scans(26), filter scans(26), filters(64)]\n",
        "    tf.keras.layers.MaxPooling2D(2,2),                                              # [batch, 15-3+1(13), 13, 64]\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),                         # [batch, filter scans(11), filter scans(11), filters(128)]\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),                                             # [batch, 11-3+1(5), 5, 128]\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),                         # [batch, filter scans(3), filter scans(3), filters(256)]\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),                                             # [batch, 3-3+1(1), 1, 256]\n",
        "    tf.keras.layers.Flatten(),                                                      # [batch, 1x1x256(256)]\n",
        "    tf.keras.layers.Dense(128, activation='relu'),                                  # [batch, dnn_units(128)]\n",
        "    tf.keras.layers.Dense(26, activation='softmax')                                 # [batch, dnn_units(26)]\n",
        "])\n",
        "\n",
        "sign_model.compile(optimizer='Adam', \n",
        "                   loss='sparse_categorical_crossentropy', \n",
        "                   metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cGG9OMSHLVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. NLP Word Embeddings\n",
        "\n",
        "bbc_model = tf.keras.Sequential([                                                     # [batch, sentence_length]\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_dim, input_length=sentence_length),   # [batch, sentence_length, embed_dimension]\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),                                         # [batch, embed_dimension]     avg embed_val per sentence\n",
        "    tf.keras.layers.Dense(24, activation='relu'),                                     # [batch, dnn_units(24)]\n",
        "    tf.keras.layers.Dense(6, activation='softmax')                                    # [batch, dnn_units(6)]\n",
        "])\n",
        "\n",
        "bbc_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',  \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxztxf93Iewk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. NLP Transfer Learning with Pretrained Word Embeddings\n",
        "\n",
        "# embed_matrix: [vocab, embed_dimension]\n",
        "glove_model = tf.keras.Sequential([                         # [batch, sentence_length]\n",
        "    tf.keras.layers.Embedding(vocab, \n",
        "                              embed_dimension, \n",
        "                              input_length=sentence_length, \n",
        "                              weights=[embed_matrix], \n",
        "                              trainable=False),             # [batch, sentence_length, embed_dimension]\n",
        "    tf.keras.layers.Dropout(0.2),                           # [batch, sentence_length, embed_dimension]\n",
        "    tf.keras.layers.Conv1D(64, 5, activation='relu'),       # [batch, filter scans(12), filters(64)]\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4),              # [batch, filter scans(3), filters(64)]  stride=pool_size=4\n",
        "    tf.keras.layers.LSTM(64),                               # [batch, lstm_units(64)]\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')          # [batch, dnn_units(1)]\n",
        "])\n",
        "\n",
        "glove_model.compile(optimizer='adam',\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q99lTepgj6ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Â 5. NLP LSTM with Regularisation\n",
        "\n",
        "# embed_matrix: [total_words, sequence_length]\n",
        "shakespeare_model = Sequential(                                                                # [batch, sentence_length(10)]\n",
        "    tf.keras.layers.Embedding(total_words, 100, input_length=10),                              # [batch, sentence_length(10), embed_dimension(100)]\n",
        "    tf.keras.layers.Bidirectional(LSTM(150, return_sequences = True)),                         # [batch, sentence_length(10), lstm_unitsx2(300)]\n",
        "    tf.keras.layers.Dropout(0.2),                                                              # [batch, 10, 300]\n",
        "    tf.keras.layers.LSTM(100),                                                                 # [batch, lstm_units(100)]\n",
        "    tf.keras.layers.Dense(1605, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # [batch, dnn_units(1605)]\n",
        "    tf.keras.layers.Dense(3211, activation='softmax')                                          # [batch, dnn_units(3211)]\n",
        ")\n",
        "\n",
        "shakespeare_model.compile(optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB1rgstB-FMd",
        "colab_type": "text"
      },
      "source": [
        "### Regularisation\n",
        "\n",
        "**Image Augmentation**\n",
        "\n",
        "Create additional training data with more variation to reduce overfitting. Use existing training images as basis for new images by manipulating them with shifts, zooms, shears, rotations, flips.\n",
        "\n",
        "*   Rotate\n",
        "*   Shift\n",
        "*   Shear\n",
        "*   Zoom\n",
        "*   Flip\n",
        "\n",
        "Example:\n",
        "train_image_datagen = ImageDataGenerator(rescale=1/255.,\n",
        "                                    rotation_range=40,\n",
        "                                    width_shift_range=0.2,\n",
        "                                    height_shift_range=0.2,\n",
        "                                    shear_range=0.2,\n",
        "                                    zoom_range=0.2,\n",
        "                                    horizontal_flip=True,\n",
        "                                    fill_mode='nearest')\n",
        "\n",
        "**Drop Out Layer**\n",
        "\n",
        "Assign a zero value to a random selection of weights in DNN layer.\n",
        "\n",
        "Example:\n",
        "tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "\n",
        "**L1, L2 Regularisation on Dense Layer**\n",
        "\n",
        "Apply regularisation factor in loss function for a given layer of DNN.\n",
        "\n",
        "L1: absolute value of weights: some weights can be zero, many solutions, better for less complex functions\n",
        "\n",
        "L2: squared value of weights: weights all positive, but small, only one solution, can fit a complex function\n",
        "\n",
        "Example:\n",
        "Dense(num_units, activation='relu', kernel_regularizer=regularizers.l2(0.01)\n"
      ]
    }
  ]
}