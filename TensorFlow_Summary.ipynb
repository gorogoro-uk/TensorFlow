{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow Summary.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRlnTlp0mGJY2uvHkL4KUx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gorogoro-uk/TensorFlow/blob/master/TensorFlow_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA7cyM5aGPYf",
        "colab_type": "text"
      },
      "source": [
        "**TensorFlow Summary**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bORh2JamG7Dt",
        "colab_type": "text"
      },
      "source": [
        "**Data Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2d0FcBuGM_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. tensorflow dataset\n",
        "# eg MNIST\n",
        "# use datasets directly in model.fit()\n",
        "\n",
        "# 1.1 get data directly and use tf methods to split into test/train, data/label subsets\n",
        "import tensorflow as tf\n",
        "mnist_data = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist_data.load_data()\n",
        "\n",
        "\n",
        "# 1.2 reshape train/test tensors\n",
        "# standarise pixel values to range 0-1\n",
        "import tensorflow as tf\n",
        "mnist_data = tf.keras.datasets.mnist\n",
        "(train_images, train_labels),(test_images, test_labels) = mnist_data.load_data()\n",
        "\n",
        "train_images = train_images.reshape(60000, 28, 28, 1)\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "train_images= train_images/255.0\n",
        "test_images = test_images/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe45MI8vKkTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. image zip file sourced from internet with Image Data Generator\n",
        "# eg happy/sad images\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# prepare directories\n",
        "BASE = Path(os.getcwd()) / 'happysad'\n",
        "HS_DATA = BASE / 'hs_data'\n",
        "ZIP_DEST = BASE / 'happy-or-sad.zip'\n",
        "ZIP_URL = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip\"\n",
        "if not os.path.exists(BASE):\n",
        "    os.mkdir(BASE)\n",
        "if not os.path.exists(HS_DATA):\n",
        "    os.mkdir(HS_DATA)\n",
        "\n",
        "# download data file & unzip\n",
        "urllib.request.urlretrieve(ZIP_URL, ZIP_DEST)\n",
        "zip_ref = zipfile.ZipFile(ZIP_DEST, 'r')\n",
        "zip_ref.extractall(HS_DATA)\n",
        "zip_ref.close()\n",
        "\n",
        "# image data generator, flow from directory\n",
        "# creates batches of images to feed to model\n",
        "# label data is created automatically based on directory structure\n",
        "# image data generator is passed to model.fit()\n",
        "image_data_gen = ImageDataGenerator(rescale=1/255.0)\n",
        "\n",
        "train_data_gen = image_data_gen.flow_from_directory(\n",
        "    HS_DATA,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=10,\n",
        "    class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i21BBrNJMA_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. image zip file sourced from internet with Image Data Generator\n",
        "# manually create train/test split and move to directory\n",
        "# eg cats/dogs\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import random\n",
        "from shutil import copyfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# prepare data directories\n",
        "BASE = Path(os.getcwd()) / 'catdog'   # base directory\n",
        "ZIP_DEST = BASE / 'cats_dogs.zip'     # zip file destination\n",
        "CAT_SOURCE = BASE / 'PetImages/Cat'\n",
        "DOG_SOURCE = BASE / 'PetImages/Dog'\n",
        "TRAIN_DEST = BASE / 'train'           # training images\n",
        "TEST_DEST = BASE / 'test'             # testing images\n",
        "TRAIN_CAT = BASE / 'train/cat'        # cat training images\n",
        "TEST_CAT = BASE / 'test/cat'          # cat testing images\n",
        "TRAIN_DOG = BASE / 'train/dog'        # dog training images\n",
        "TEST_DOG = BASE / 'test/dog'          # dog testing images\n",
        "if not os.path.exists(BASE):\n",
        "    os.mkdir(BASE)\n",
        "if not os.path.exists(TRAIN_DEST):\n",
        "    os.mkdir(TRAIN_DEST)\n",
        "if not os.path.exists(TEST_DEST):\n",
        "    os.mkdir(TEST_DEST)\n",
        "if not os.path.exists(TRAIN_CAT):\n",
        "    os.mkdir(TRAIN_CAT)\n",
        "if not os.path.exists(TRAIN_DOG):\n",
        "    os.mkdir(TRAIN_DOG)\n",
        "if not os.path.exists(TEST_CAT):\n",
        "    os.mkdir(TEST_CAT)\n",
        "if not os.path.exists(TEST_DOG):\n",
        "    os.mkdir(TEST_DOG)\n",
        "\n",
        "# download & unzip data\n",
        "URL = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\"\n",
        "urllib.request.urlretrieve(URL, ZIP_DEST)\n",
        "zip_ref = zipfile.ZipFile(ZIP_DEST, 'r')\n",
        "zip_ref.extractall(BASE)\n",
        "zip_ref.close()\n",
        "\n",
        "# split data into train & test\n",
        "TRAIN_SIZE = 0.90\n",
        "\n",
        "def split_data(source, train, test, split):\n",
        "    \"\"\" shuffle images, copy to directory, split into train/test \"\"\"\n",
        "\n",
        "    # list of image file names\n",
        "    files = []\n",
        "    for filename in os.listdir(source):\n",
        "        file = source / filename\n",
        "        if os.path.getsize(file) > 0:\n",
        "            files.append(filename)\n",
        "        else:\n",
        "            print(filename + \" is zero length, so ignoring.\")\n",
        "\n",
        "    # shuffle dataset images\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "\n",
        "    # define train, test split\n",
        "    train_length = int(len(files) * split)\n",
        "    test_length = int(len(files) - train_length)\n",
        "    train_set = shuffled_set[0:train_length]\n",
        "    test_set = shuffled_set[-test_length:]\n",
        "\n",
        "    # move files to train or test directory\n",
        "    for filename in train_set:\n",
        "        this_file = source / filename\n",
        "        destination = train / filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "    for filename in test_set:\n",
        "        this_file = source / filename\n",
        "        destination = test / filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "split_data(CAT_SOURCE, TRAIN_CAT, TEST_CAT, TRAIN_SIZE)\n",
        "split_data(DOG_SOURCE, TRAIN_DOG, TEST_DOG, TRAIN_SIZE)\n",
        "\n",
        "# image data generator, flow from directory\n",
        "# data augmentation: rotate, shift, shear, zoom, flip\n",
        "# augmentation not required on test images\n",
        "# define batch, image size, create binary labels based on directory\n",
        "# image data generator is passed to model.fit()\n",
        "train_image_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                          rotation_range=40,\n",
        "                                          width_shift_range=0.2,\n",
        "                                          height_shift_range=0.2,\n",
        "                                          shear_range=0.2,\n",
        "                                          zoom_range=0.2,\n",
        "                                          horizontal_flip=True,\n",
        "                                          fill_mode='nearest')\n",
        "train_datagen = train_image_datagen.flow_from_directory(TRAIN_DEST,\n",
        "                                                    batch_size=100,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "test_image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = test_image_datagen.flow_from_directory(TEST_DEST,\n",
        "                                                    batch_size=100,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQG5AuzlONMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. image zip file sourced from internet with Image Data Generator\n",
        "# separate train & test datasets\n",
        "# eg. horse/human images\n",
        "\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#  prepare data directories\n",
        "CWD = Path(os.getcwd())\n",
        "BASE = CWD / 'horsehuman'\n",
        "TRAIN = BASE / 'train'\n",
        "TEST = BASE / 'test'\n",
        "TRAIN_HORSE = TRAIN / 'horses'\n",
        "TRAIN_HUMAN = TRAIN / 'humans'\n",
        "TEST_HORSE = TEST / 'horses'\n",
        "TEST_HUMAN = TEST / 'humans'\n",
        "if not os.path.exists(BASE):\n",
        "    os.mkdir(BASE)\n",
        "if not os.path.exists(TRAIN):\n",
        "    os.mkdir(TRAIN)\n",
        "if not os.path.exists(TEST):\n",
        "    os.mkdir(TEST)\n",
        "\n",
        "# get train & test datasets\n",
        "TRAIN_URL = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
        "TEST_URL = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\n",
        "ZIP_TRAIN = TRAIN / 'horse-or-human.zip'\n",
        "ZIP_TEST = TEST / 'validation-horse-or-human.zip'\n",
        "\n",
        "urllib.request.urlretrieve(TRAIN_URL,TRAIN / ZIP_TRAIN)\n",
        "urllib.request.urlretrieve(TEST_URL,TEST / ZIP_TEST)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(ZIP_TRAIN, 'r')\n",
        "zip_ref.extractall(TRAIN)\n",
        "zip_ref.close()\n",
        "zip_ref = zipfile.ZipFile(ZIP_TEST, 'r')\n",
        "zip_ref.extractall(TEST)\n",
        "zip_ref.close()\n",
        "\n",
        "# image data generator, flow from directory\n",
        "# data augmentation: rescale, rotate, shift, shear, zoom, flip\n",
        "# define batch, image size, create binary labels based on directory\n",
        "# image data generator is passed to model.fit()\n",
        "# augmentation not required on test images\n",
        "train_image_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                          rotation_range=40,\n",
        "                                          width_shift_range=0.2,\n",
        "                                          height_shift_range=0.2,\n",
        "                                          shear_range=0.2,\n",
        "                                          zoom_range=0.2,\n",
        "                                          horizontal_flip=True)\n",
        "\n",
        "train_datagen = train_image_datagen.flow_from_directory(TRAIN,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "test_image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen = test_image_datagen.flow_from_directory(TEST,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRmxIUkjPSp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. csv file sourced from internet with Image Data Generator\n",
        "# data & label extracted from csv file\n",
        "# separate train & test files\n",
        "# eg. sign MNIST images\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# prepare directories\n",
        "CWD = Path(os.getcwd())\n",
        "TRAIN_FILE = CWD / 'signlanguage/sign_mnist_train.csv'\n",
        "TEST_FILE = CWD / 'signlanguage/sign_mnist_test.csv'\n",
        "\n",
        "# read data from csv file, line by line\n",
        "# image pixel values flattened in one row, reshape for use\n",
        "def prepare_data(file_name):\n",
        "    with open(file_name) as data_file:\n",
        "        csv_reader = csv.reader(data_file, delimiter=',')\n",
        "        first_line = True\n",
        "        label_temp = []\n",
        "        image_temp = []\n",
        "        for row in csv_reader:\n",
        "            if first_line:\n",
        "                first_line = False\n",
        "            else:\n",
        "                label_temp.append(row[0])\n",
        "                image_row = row[1:785]\n",
        "                image_array = np.reshape(image_row, (28,28))\n",
        "                image_temp.append(image_array)\n",
        "    labels = np.array(label_temp).astype('float')\n",
        "    images = np.array(image_temp).astype('float')\n",
        "    return images, labels\n",
        "\n",
        "x_train, y_train = prepare_data(TRAIN_FILE)\n",
        "x_test, y_test = prepare_data(TEST_FILE)\n",
        "x_train = np.expand_dims(x_train, axis=3)   # make [28, 28, 1]\n",
        "x_test = np.expand_dims(x_test, axis=3)     # make [28, 28, 1]\n",
        "\n",
        "# image data generator, flow\n",
        "# data augmentation: rescale, rotate, shift, shear, zoom, flip\n",
        "# define batch & data/label datsets\n",
        "# image data generator is passed to model.fit()\n",
        "# augmentation not required on test images\n",
        "train_image_datagen = ImageDataGenerator(rescale=1/255.,\n",
        "                                    rotation_range=40,\n",
        "                                    width_shift_range=0.2,\n",
        "                                    height_shift_range=0.2,\n",
        "                                    shear_range=0.2,\n",
        "                                    zoom_range=0.2,\n",
        "                                    horizontal_flip=True,\n",
        "                                    fill_mode='nearest')\n",
        "train_datagen = train_image_datagen.flow(x_train, y_train, batch_size=32)\n",
        "\n",
        "test_image_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = test_image_datagen.flow(x_test, y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSVSDg-nWFcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6. csv file sourced from internet\n",
        "# data & label extracted from csv file\n",
        "# eg. BBC news texts\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# get dataset\n",
        "BBC = Path(os.getcwd()) / 'bbc'\n",
        "if not os.path.exists(BBC):\n",
        "    os.mkdir(BBC)\n",
        "BBC_FILE = BBC / 'bbc-text.csv'\n",
        "BBC_URL = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv'\n",
        "urllib.request.urlretrieve(BBC_URL,BBC_FILE)\n",
        "\n",
        "# read csv file: extract data & labels datasets\n",
        "# reomve stopwords\n",
        "# stopwords python list from external source \n",
        "sentences = []\n",
        "labels = []\n",
        "with open(BBC_FILE, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        labels.append(row[0])\n",
        "        sentence = row[1]\n",
        "        for word in stopwords:\n",
        "            token = \" \" + word + \" \"\n",
        "            sentence = sentence.replace(token, \" \")\n",
        "            sentence = sentence.replace(\"  \", \" \")\n",
        "        sentences.append(sentence)\n",
        "\n",
        "# split into train/test\n",
        "train_size = int(len(sentences) * train_ratio)\n",
        "train_sentences = sentences[:train_size]\n",
        "test_sentences = sentences[train_size:]\n",
        "train_labels = labels[:train_size]\n",
        "test_labels = labels[train_size:]\n",
        "\n",
        "# tokenize train data\n",
        "# create word_index, convert to sequences, pad\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "train_seq = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_pad = pad_sequences(train_seq, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "# tokenize test data with same tokenizer used on train data\n",
        "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_pad = pad_sequences(test_seq, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "# tokenize labels\n",
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "# convert to Numpy arrays\n",
        "train_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "test_label_seq = np.array(label_tokenizer.texts_to_sequences(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Khvz9FXoOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 7. csv file sourced from internet\n",
        "# data & label extracted from csv file\n",
        "# eg. social media texts\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# get dataset\n",
        "CWD = Path(os.getcwd())\n",
        "BASE = CWD / 'glove'\n",
        "if not os.path.exists(BASE):\n",
        "    os.mkdir(BASE)\n",
        "DATA_FILE = BASE / 'training_cleaned.csv'\n",
        "DATA_URL = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv'\n",
        "urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
        "\n",
        "# read csv file: extract data & label\n",
        "num_sent = 0\n",
        "corpus = []\n",
        "with open(DATA_FILE) as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "        row_item = []\n",
        "        row_item.append(row[5])\n",
        "        row_label = row[0]\n",
        "        if row_label == '0':\n",
        "            row_item.append(0)\n",
        "        else:\n",
        "            row_item.append(1)\n",
        "        num_sent = num_sent + 1\n",
        "        corpus.append(row_item)\n",
        "\n",
        "# shuffle and split into data/label\n",
        "random.shuffle(corpus)\n",
        "sentences=[]\n",
        "labels=[]\n",
        "for x in range(train_size):\n",
        "    sentences.append(corpus[x][0])\n",
        "    labels.append(corpus[x][1])\n",
        "\n",
        "# tokenize data\n",
        "# create word_index, convert to sequences, pad\n",
        "train_tokenizer = Tokenizer()\n",
        "train_tokenizer.fit_on_texts(sentences)\n",
        "word_index = train_tokenizer.word_index\n",
        "train_seqs = train_tokenizer.texts_to_sequences(sentences)\n",
        "train_pad_seqs = pad_sequences(train_seqs, maxlen=max_length, padding=pad_type, truncating=trunc_type)\n",
        "\n",
        "# split into train/test, convert to numpy arrays\n",
        "split = int(test_ratio * train_size)\n",
        "train_data = np.array(train_pad_seqs[split:train_size])\n",
        "train_labels = np.array(labels[split:train_size])\n",
        "test_data = np.array(train_pad_seqs[0:split])\n",
        "test_labels = np.array(labels[0:split])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZoIL8IsZe-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 8. csv file sourced from internet\n",
        "# data extracted from txt file\n",
        "# create n-gram sequences & labels\n",
        "# eg. Shakespeare's sonnets\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.utils as ku\n",
        "import numpy as np\n",
        "\n",
        "# get data\n",
        "BASE = Path(os.getcwd()) / 'shakespeare'\n",
        "SONNET_FILE = 'shakespeare/sonnets.txt'\n",
        "SONNET_URL = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt'\n",
        "SHAKESPEARE_MODEL_FILE = 'shakespeare/shakespeare_model'\n",
        "if not os.path.exists(BASE):\n",
        "    os.mkdir(BASE)\n",
        "urllib.request.urlretrieve(SONNET_URL,SONNET_FILE)\n",
        "\n",
        "\n",
        "# tokenize data\n",
        "# create world_list, sequences, pad, convert to numpy array\n",
        "data = open(SONNET_FILE).read()\n",
        "corpus = data.lower().split(\"\\n\")       # list of strings\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1  # oov token is extra\n",
        "\n",
        "# create n-grams of all lengths from 1 to max line length\n",
        "input_seqs = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_seq = token_list[:i+1]\n",
        "        input_seqs.append(n_gram_seq)\n",
        "\n",
        "max_seq_len = max([len(x) for x in input_seqs])\n",
        "input_seqs = np.array(pad_sequences(input_seqs, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "# create predictors and label datasets\n",
        "# label is last word in each n-gram sequence\n",
        "predictors, label = input_seqs[:,:-1],input_seqs[:,-1]\n",
        "label = ku.to_categorical(label, num_classes=total_words)    # one hot encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5oFfNbMaURx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 9. time series data made with functions\n",
        "# built form components: base, trend, seasonality, noise\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "# plot time series\n",
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    \"\"\"plot a series\"\"\"\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "\n",
        "# linear trend\n",
        "def trend(time, slope=0):\n",
        "    \"\"\"simple linear trend\"\"\"\n",
        "    return slope * time\n",
        "\n",
        "# recurring seasonal pattern\n",
        "def seasonal_pattern(season_time):\n",
        "    \"\"\"arbitrary pattern per period\"\"\"\n",
        "    return np.where(season_time < 0.1,\n",
        "                    np.cos(season_time * 7 * np.pi),\n",
        "                    1 / np.exp(5 * season_time))\n",
        "\n",
        "# seasonality\n",
        "def seasonality(time, period, amplitude=1, phase=0):\n",
        "    \"\"\"seasonal pattern over full dataset\"\"\"\n",
        "    season_time = ((time + phase) % period) / period\n",
        "    return amplitude * seasonal_pattern(season_time)\n",
        "\n",
        "# random noise simulator\n",
        "def noise(time, noise_level=1, seed=None):\n",
        "    \"\"\"random noise\"\"\"\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    return rnd.randn(len(time)) * noise_level\n",
        "\n",
        "# create timebase vector\n",
        "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
        "\n",
        "# constants\n",
        "baseline = 10\n",
        "amplitude = 40\n",
        "slope = 0.01\n",
        "noise_level = 2\n",
        "\n",
        "# Create the series\n",
        "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
        "series += noise(time, noise_level, seed=42)\n",
        "\n",
        "# create train/test datasets\n",
        "split_time = 1100\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cGWnjXFDfoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10. Windowed Dataset for Time Series Data\n",
        "# manufacture data & label datasets from time series\n",
        "# sliding window of data to predict next value\n",
        "\n",
        "# define windowed dataset\n",
        "# crete datset with right dimensions\n",
        "# define window size, window shift, stride, drop partial windows\n",
        "# convert to arrays, create data & label portions\n",
        "# shuffle, batch, prefetch\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "# create windowed dataset\n",
        "train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "# reference windowed dataset in model.fit()\n",
        "sunspot_model_hist = sunspot_model.fit(train_set,\n",
        "                                       epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}